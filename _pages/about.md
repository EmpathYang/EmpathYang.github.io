---
permalink: /
title: "Welcome :)"
excerpt: "Welcome :)"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hello there! I'm Ke Yang (杨可), currently a second-year Ph.D. at UIUC under the guidance of Professor [ChengXiang Zhai](http://czhai.cs.illinois.edu/). I obtain my bachelor's degree from Tsinghua University. I interned with Professor [Heng Ji](http://blender.cs.illinois.edu/hengji.html)'s group at UIUC in 2022 summer, and at Amazon AWS in 2024 summer.

I work on **intelligent agents**, **language models**, **graph neural networks**, and **multimodality foundation models** (of top interest). I'm also keen on **NLP for societal benefit**.

During the winter break of 2022, I collaborated with two of my undergrad classmates and created [*Zempath*](#jump), an online social platform incorporated with our trained chatbots with distinctive personalities.

<style>
table, td, th, tr {
   border: none!important;
   font-size: 14px;
}
</style>
<h2><span>Selected Publications</span></h2>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2024</div><img src='/images/TinyHelen.png' alt="yang2024tinyhelenscurriculumtrainingevaluating" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>TinyHelen's First Curriculum: Training and Evaluating Tiny Language Models in a Simpler Language Environment</h3>
<span style="font-family: Georgia, serif; font-size: smaller;"><b>Ke Yang</b>, Volodymyr Kindratenko, ChengXiang Zhai</span>
<br>
<a href='https://arxiv.org/abs/2501.00522'><button class="paper-btn">PAPER</button></a> <a href='https://github.com/EmpathYang/TinyHelen'><button class="code-btn">CODE</button></a>
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">We train and evaluate tiny language models using a text dataset with simplified vocabularies and linguistic structures, mimicking how children learn language through simplified environments as part of their initial curriculum.</em>
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2025</div><img src='/images/AgentOccam-overview.png' alt="yang2024agentoccamsimplestrongbaseline" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents</h3>
<span style="font-family: Georgia, serif; font-size: smaller;"><b>Ke Yang</b>, Yao Liu, Sapana Chaudhary, Rasool Fakoor, Pratik Chaudhari, George Karypis, Huzefa Rangwala</span>
<br>
<a href='https://arxiv.org/abs/2410.13825'><button class="paper-btn">PAPER</button></a> <a href='https://github.com/amazon-science/AgentOccam'><button class="code-btn">CODE</button></a>
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">AgentOccam surpasses the previous state-of-the-art and concurrent LLM-based web agent with its observation and action space alignment. We achieve this without using in-context examples, new agent roles, online feedback or search strategies.</em>
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024 D&B Track</div><img src='/images/Prejudice-framework.png' alt="2024prejudice" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>Bias and Volatility: A Statistical Framework for Evaluating Large Language Model's Stereotypes and the Associated Generation Inconsistency</h3>
<span style="font-family: Georgia, serif; font-size: smaller;">Yiran Liu\*, <b>Ke Yang</b>\*, Zehan Qi, Xiao Liu, Yang Yu, ChengXiang Zhai (* indicates equal contributions)</span>
<br>
<a href='https://arxiv.org/abs/2402.15481'><button class="paper-btn">PAPER</button></a> <a href='https://github.com/EmpathYang/Prejudice-Volatility-Framework'><button class="code-btn">CODE</button></a> <a href='/files/PVF_note.pdf'><button class="note-btn">NOTE</button></a> <a href='files/BVF.pdf'><button class="slide-btn">SLIDE</button></a>
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">Bias-Volatility Framework measures discrimination in models by considering both their consistently biased preference and preference variation across contexts.</em>
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR Workshop 2024</div><img src='/images/Wizard-agent.png' alt="yang2024llm" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents</h3>
<span style="font-family: Georgia, serif; font-size: smaller;"><b>Ke Yang</b>\*, Jiateng Liu\*, John Wu, Chaoqi Yang, Yi R. Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang, Heng Ji, ChengXiang Zhai  (* indicates equal contributions)</span>
<br>
<a href='https://arxiv.org/abs/2401.00812'><button class="paper-btn">PAPER</button></a>
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">The Wizard survey explores the synergy between code and large language models (LLMs), highlighting how code empowers LLMs and benefits LLM when they serve as intelligent agents. We emphasized code’s readability, symbolic abstraction, and graph structure, presenting it as a valuable component in LLMs’ training corpus.</em>
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 23</div><img src='/images/ADEPT-framework.png' alt="yang2022adept" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<h3>ADEPT: A DEbiasing PrompT Framework</h3>
<span style="font-family: Georgia, serif; font-size: smaller;"><b>Ke Yang</b>, Charles Yu, Yi Fung, Manling Li, Heng Ji</span>
<br>
<a href='https://arxiv.org/abs/2211.05414'><button class="paper-btn">PAPER</button></a> <a href='https://github.com/EmpathYang/ADEPT/'><button class="code-btn">CODE</button></a> <a href='files/ADEPT.pdf'><button class="slide-btn">SLIDE</button></a>
<br>
<em style="font-family: 'Times New Roman', Times, serif; font-size: smaller;">ADEPT introduces a novel debaising loss function based on counterfactual bias and manifold learning insights. "Prompt" here refers to prompt-tuning (peft) rather than prompt-engineering.</em>
</div>
</div>

<h2>
    <img src="/images/Zempath.png" alt="Icon" style="display: inline-block; vertical-align: middle; width: 50px;">
    <span id='jump'>Zempath</span> 
</h2>

In the promotional video for Zempath, we unveil our driving inspirations and fundamental principles. We showcase the seamless user experience of engaging in chats, posting either anonymously or under one's real name, indulging in conversations with our personalized chatbots, and forging new connections with like-minded individuals. Let's delve into a snippet from this captivating video:

![Zempath](/images/Zempath_display.gif)

Miscellaneous
------
I am an amateur novelist, [painter](/images/hey_you.jpg), and photographer. I take photos of [cats](/images/cat.jpg), [my sister](/images/my_cool_sister.jpg), [grandparents](/images/my_grandparents.jpg), [friends](/images/on_my_21th_birthday.png), [campus](/images/campus.png), etc., in my spare time.
